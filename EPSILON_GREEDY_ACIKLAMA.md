# EPSILON-GREEDY, EPSILON ve EPSILON_DECAY - DETAYLI AÃ‡IKLAMA

## ğŸ¯ EPSILON-GREEDY NEDÄ°R?

**Epsilon-greedy** = Exploration (keÅŸif) ve Exploitation (sÃ¶mÃ¼rme) dengesini saÄŸlayan bir strateji.

### Basit AÃ§Ä±klama

Agent iki ÅŸey yapabilir:
1. **Exploration (KeÅŸif)**: Random aksiyon seÃ§, yeni ÅŸeyler Ã¶ÄŸren
2. **Exploitation (SÃ¶mÃ¼rme)**: En iyi aksiyonu seÃ§, Ã¶ÄŸrendiÄŸini kullan

**Epsilon-greedy** bu ikisini dengeler!

---

## ğŸ“Š EPSILON NEDÄ°R?

**Epsilon (Îµ)** = Random aksiyon seÃ§me olasÄ±lÄ±ÄŸÄ±

### DeÄŸer AralÄ±ÄŸÄ±

```
Îµ âˆˆ [0, 1]
```

- **Îµ = 1.0** â†’ %100 random (tamamen keÅŸif)
- **Îµ = 0.5** â†’ %50 random, %50 en iyi
- **Îµ = 0.1** â†’ %10 random, %90 en iyi
- **Îµ = 0.0** â†’ %100 en iyi (tamamen sÃ¶mÃ¼rme, random yok)

### Kodda

```python
self.epsilon = 1.0  # BaÅŸlangÄ±Ã§ deÄŸeri
self.epsilon_min = 0.1  # Minimum deÄŸer (asla bu deÄŸerin altÄ±na dÃ¼ÅŸmez)
```

---

## ğŸ² EPSILON-GREEDY NASIL Ã‡ALIÅIR?

### Kod Ä°ncelemesi

```python
def choose_action(self, state, training=True):
    state_idx = self.get_state_index(state)
    
    if training and np.random.rand() < self.epsilon:
        # Exploration: Random aksiyon seÃ§
        return self.env.action_space.sample()
    else:
        # Exploitation: En iyi aksiyonu seÃ§
        return int(np.argmax(self.q_table[state_idx]))
```

### AdÄ±m AdÄ±m Ne Oluyor?

1. **Random sayÄ± Ã¼ret**: `np.random.rand()` â†’ 0 ile 1 arasÄ± sayÄ±
2. **KarÅŸÄ±laÅŸtÄ±r**: `random < epsilon` mi?
   - **EVET** â†’ Random aksiyon seÃ§ (exploration)
   - **HAYIR** â†’ En iyi aksiyonu seÃ§ (exploitation)

### Ã–rnek Senaryolar

#### Senaryo 1: Îµ = 1.0 (BaÅŸlangÄ±Ã§)

```python
random = 0.7
epsilon = 1.0
0.7 < 1.0 â†’ TRUE â†’ Random aksiyon seÃ§
```

**SonuÃ§:** %100 random (her zaman exploration)

#### Senaryo 2: Îµ = 0.5 (Orta)

```python
random = 0.3
epsilon = 0.5
0.3 < 0.5 â†’ TRUE â†’ Random aksiyon seÃ§

random = 0.7
epsilon = 0.5
0.7 < 0.5 â†’ FALSE â†’ En iyi aksiyonu seÃ§
```

**SonuÃ§:** %50 random, %50 en iyi

#### Senaryo 3: Îµ = 0.1 (Sonraki AÅŸamalar)

```python
random = 0.05
epsilon = 0.1
0.05 < 0.1 â†’ TRUE â†’ Random aksiyon seÃ§

random = 0.15
epsilon = 0.1
0.15 < 0.1 â†’ FALSE â†’ En iyi aksiyonu seÃ§
```

**SonuÃ§:** %10 random, %90 en iyi

#### Senaryo 4: Îµ = 0.0 (Test Modu)

```python
random = 0.5
epsilon = 0.0
0.5 < 0.0 â†’ FALSE â†’ En iyi aksiyonu seÃ§
```

**SonuÃ§:** %100 en iyi (hiÃ§ random yok)

---

## ğŸ“‰ EPSILON DECAY (AZALMA) NEDÄ°R?

**Epsilon Decay** = Epsilon deÄŸerinin zamanla azalmasÄ±

### Neden AzalÄ±yor?

1. **BaÅŸlangÄ±Ã§ta (Îµ = 1.0)**:
   - Agent hiÃ§bir ÅŸey bilmiyor
   - Ã‡ok keÅŸif yapmalÄ± (exploration)
   - OrtamÄ± tanÄ±malÄ±

2. **SonralarÄ± (Îµ â†’ 0.1)**:
   - Agent Ã¶ÄŸrenmeye baÅŸladÄ±
   - Daha Ã§ok sÃ¶mÃ¼rmeli (exploitation)
   - Ã–ÄŸrendiÄŸini kullanmalÄ±

### Ä°ki YÃ¶ntem

#### 1. Multiplicative Decay (Basit)

```python
self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
```

**NasÄ±l Ã‡alÄ±ÅŸÄ±r?**

```python
epsilon = 1.0
epsilon_decay = 0.995
epsilon_min = 0.1

Episode 0:  epsilon = 1.0
Episode 1:  epsilon = 1.0 Ã— 0.995 = 0.995
Episode 2:  epsilon = 0.995 Ã— 0.995 = 0.990
Episode 3:  epsilon = 0.990 Ã— 0.995 = 0.985
...
Episode 100: epsilon = 0.605
Episode 200: epsilon = 0.366
Episode 300: epsilon = 0.221
Episode 400: epsilon = 0.134
Episode 500: epsilon = 0.100 (epsilon_min'e ulaÅŸtÄ±)
```

**Grafik:**
```
Îµ
1.0 |â–ˆâ–ˆâ–ˆâ–ˆ
    |  â–ˆâ–ˆâ–ˆâ–ˆ
    |     â–ˆâ–ˆâ–ˆâ–ˆ
    |        â–ˆâ–ˆâ–ˆâ–ˆ
0.1 |              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Episode
    0    100   200   300   400   500
```

**Avantaj:** Basit, hÄ±zlÄ±
**Dezavantaj:** Ani dÃ¼ÅŸÃ¼ÅŸler olabilir

#### 2. Reverse Sigmoid Decay (Senin Kodun)

```python
def reverse_sigmoid_decay(t, epsilon_initial, epsilon_min, k, t0):
    return epsilon_min + (epsilon_initial - epsilon_min) / (1 + np.exp(k * (t - t0)))
```

**Parametreler:**
- `t`: Mevcut episode numarasÄ±
- `epsilon_initial`: BaÅŸlangÄ±Ã§ epsilon (1.0)
- `epsilon_min`: Minimum epsilon (0.1)
- `k`: Decay hÄ±zÄ± (ne kadar hÄ±zlÄ± azalacak)
- `t0`: Inflection point (en hÄ±zlÄ± azalmanÄ±n olduÄŸu episode)

**NasÄ±l Ã‡alÄ±ÅŸÄ±r?**

```python
epsilon_initial = 1.0
epsilon_min = 0.1
k = 0.01
t0 = 25

Episode 0:   epsilon = 1.0 / (1 + exp(0.01 Ã— (0 - 25))) â‰ˆ 0.94
Episode 10:  epsilon = 1.0 / (1 + exp(0.01 Ã— (10 - 25))) â‰ˆ 0.82
Episode 25:  epsilon = 1.0 / (1 + exp(0.01 Ã— (25 - 25))) = 0.5  â† Inflection point!
Episode 40:  epsilon = 1.0 / (1 + exp(0.01 Ã— (40 - 25))) â‰ˆ 0.18
Episode 50:  epsilon = 1.0 / (1 + exp(0.01 Ã— (50 - 25))) â‰ˆ 0.12
Episode 100: epsilon â‰ˆ 0.1 (epsilon_min'e yaklaÅŸtÄ±)
```

**Grafik (S Åeklinde):**
```
Îµ
1.0 |â–ˆâ–ˆâ–ˆâ–ˆ
    |  â–ˆâ–ˆâ–ˆâ–ˆ
    |     â–ˆâ–ˆâ–ˆâ–ˆ
    |        â–ˆâ–ˆâ–ˆâ–ˆ
    |           â–ˆâ–ˆâ–ˆâ–ˆ
0.5 |              â—  â† Inflection point (t0)
    |                 â–ˆâ–ˆâ–ˆâ–ˆ
    |                    â–ˆâ–ˆâ–ˆâ–ˆ
0.1 |                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Episode
    0    10   20   30   40   50   100
```

**Avantaj:** Smooth geÃ§iÅŸ, kontrollÃ¼ azalma
**Dezavantaj:** Biraz daha karmaÅŸÄ±k

---

## ğŸ”„ KODDA NASIL KULLANILIYOR?

### Training SÄ±rasÄ±nda

```python
def train(self, num_episodes=1000):
    self.epsilon = self.epsilon_initial  # BaÅŸlangÄ±Ã§: 1.0
    
    for episode in range(num_episodes):
        # Episode boyunca epsilon-greedy kullan
        while not done:
            action = self.choose_action(state, training=True)
            # epsilon-greedy ile aksiyon seÃ§ildi
        
        # Episode sonunda epsilon'u azalt
        if self.use_reverse_sigmoid:
            self.epsilon = reverse_sigmoid_decay(
                episode, self.epsilon_initial, self.epsilon_min,
                self.sigmoid_k, self.sigmoid_t0
            )
        else:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
```

### Test SÄ±rasÄ±nda

```python
def test(self, num_episodes=10):
    for episode in range(num_episodes):
        while not done:
            # training=False â†’ epsilon kullanÄ±lmaz!
            action = self.choose_action(state, training=False)
            # Her zaman en iyi aksiyonu seÃ§er (greedy)
```

**Fark:**
- `training=True` â†’ Epsilon-greedy kullan (random olabilir)
- `training=False` â†’ Sadece greedy (her zaman en iyi)

---

## ğŸ“Š EPSILON DEÄERLERÄ°NÄ°N ETKÄ°SÄ°

### Tablo

| Episode | Epsilon | DavranÄ±ÅŸ | AÃ§Ä±klama |
|---------|---------|----------|----------|
| 0-10 | 1.0 â†’ 0.9 | %90-100 random | Ã‡ok keÅŸif, ortamÄ± tanÄ± |
| 10-50 | 0.9 â†’ 0.5 | %50-90 random | Hala keÅŸif, Ã¶ÄŸrenmeye baÅŸla |
| 50-100 | 0.5 â†’ 0.2 | %20-50 random | Daha Ã§ok sÃ¶mÃ¼r, az keÅŸif |
| 100+ | 0.2 â†’ 0.1 | %10-20 random | Ã‡oÄŸunlukla sÃ¶mÃ¼r, az keÅŸif |

---

## â“ SIK SORULAN SORULAR

### SORU 1: "Epsilon neden 1.0'dan baÅŸlÄ±yor?"

**CEVAP:**
"Agent baÅŸlangÄ±Ã§ta hiÃ§bir ÅŸey bilmiyor. Q-table tÃ¼m sÄ±fÄ±rlar. EÄŸer epsilon dÃ¼ÅŸÃ¼k olsaydÄ±, agent hep aynÄ± aksiyonu seÃ§erdi (Ã§Ã¼nkÃ¼ tÃ¼m Q deÄŸerleri 0, hepsi eÅŸit). Bu yÃ¼zden baÅŸta %100 random yapmalÄ±, ortamÄ± keÅŸfetmeli."

### SORU 2: "Epsilon hiÃ§ azalmasa ne olur?"

**CEVAP:**
"Agent sÃ¼rekli random hareketler yapar. Ã–ÄŸrendiÄŸi optimal policy'yi kullanamaz. Mesela 1000 episode sonra optimal yolu Ã¶ÄŸrenmiÅŸ olsa bile, hala %Îµ olasÄ±lÄ±kla random yapÄ±yor. Bu yÃ¼zden performans dÃ¼ÅŸÃ¼k kalÄ±r, tutarsÄ±z olur."

### SORU 3: "Epsilon Ã§ok hÄ±zlÄ± azalÄ±rsa ne olur?"

**CEVAP:**
"Yetersiz exploration olur. Agent ortamÄ± yeterince keÅŸfedemeden exploit etmeye baÅŸlar. Local optimum'a takÄ±labilir, optimal policy'yi bulamayabilir."

### SORU 4: "Epsilon Ã§ok yavaÅŸ azalÄ±rsa ne olur?"

**CEVAP:**
"EÄŸitim Ã§ok uzun sÃ¼rer. Agent Ã§ok fazla random yapar, Ã¶ÄŸrendiÄŸini kullanamaz. Convergence yavaÅŸ olur."

### SORU 5: "Reverse sigmoid neden daha iyi?"

**CEVAP:**
"Smooth geÃ§iÅŸ saÄŸlÄ±yor. BaÅŸta yavaÅŸ azalÄ±r (Ã§ok keÅŸif), ortada hÄ±zlÄ± azalÄ±r (hÄ±zlÄ± Ã¶ÄŸrenme), sonda tekrar yavaÅŸ azalÄ±r (stabil). Ani dÃ¼ÅŸÃ¼ÅŸler yok, daha kontrollÃ¼."

---

## ğŸ¯ Ã–ZET

### Epsilon-Greedy
- **Ne?** Exploration ve exploitation dengesi
- **NasÄ±l?** Îµ olasÄ±lÄ±kla random, 1-Îµ olasÄ±lÄ±kla en iyi

### Epsilon
- **Ne?** Random aksiyon seÃ§me olasÄ±lÄ±ÄŸÄ±
- **DeÄŸer:** 0.0 (hiÃ§ random) ile 1.0 (tamamen random) arasÄ±

### Epsilon Decay
- **Ne?** Epsilon'un zamanla azalmasÄ±
- **Neden?** BaÅŸta keÅŸif, sonra sÃ¶mÃ¼rme
- **YÃ¶ntemler:** Multiplicative veya Reverse Sigmoid

### Kod Ã–zeti

```python
# BaÅŸlangÄ±Ã§
epsilon = 1.0  # %100 random

# Her episode'da
if random() < epsilon:
    action = random_action()  # Exploration
else:
    action = best_action()    # Exploitation

# Episode sonunda
epsilon = decay(epsilon)  # Azalt
```

---

**BAÅARILAR! ğŸš€**

