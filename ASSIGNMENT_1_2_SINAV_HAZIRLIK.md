# ASSIGNMENT 1 & 2 - SIFIRDAN SINAV HAZIRLIÄI

## ğŸ“š Ä°Ã‡Ä°NDEKÄ°LER

1. [Assignment 1: Environment (chid_env.py)](#assignment-1-environment)
2. [Assignment 2: Q-Learning (assignment2_qlearning.py)](#assignment-2-q-learning)
3. [Kod AÃ§Ä±klamalarÄ±](#kod-aÃ§Ä±klamalarÄ±)
4. [SÄ±nav SorularÄ± ve CevaplarÄ±](#sÄ±nav-sorularÄ±-ve-cevaplarÄ±)

---

# ASSIGNMENT 1: ENVIRONMENT

## ğŸ¯ NE YAPIYORUZ?

Bir **Grid World** (labirent) yapÄ±yoruz. Agent (Scrat) bu labirentte dolaÅŸÄ±p goal'a (palamut) ulaÅŸmaya Ã§alÄ±ÅŸÄ±yor.

## ğŸ“ ENVIRONMENT YAPISI

### Grid Boyutu
- **7 satÄ±r Ã— 12 sÃ¼tun** = 84 hÃ¼cre
- Her hÃ¼cre bir durumu temsil ediyor

### Ã–zel HÃ¼creler

| Sembol | Ä°sim | Ne Yapar? | Reward |
|--------|------|-----------|--------|
| `.` | Empty | BoÅŸ hÃ¼cre, geÃ§ilebilir | -1 (living cost) |
| `O` | Obstacle | Buz kristali, geÃ§ilemez | -1 (Ã§arparsan) |
| `H` | Hell/Danger | DÃ¼ÅŸman, Ã¶lÃ¼rsÃ¼n | -100 (episode biter) |
| `R` | Reward | Mini Ã¶dÃ¼l | +1 |
| `L` | Lover | Scratte, bonus | +100 (ilk alÄ±ÅŸta) |
| `G` | Goal | Palamut, hedef | +100 (episode biter) |
| `A` | Agent Start | BaÅŸlangÄ±Ã§ pozisyonu | - |

### Lover Multiplier (6x Ã‡arpan)

**Ã‡OK Ã–NEMLÄ°!** Lover alÄ±rsan:
- Normal Goal: +100 â†’ **Lover ile: +600**
- Normal Danger: -100 â†’ **Lover ile: -600**

**Neden var?** Risk-reward dengesi. Lover almak avantajlÄ± ama riskli!

---

## ğŸ” STATE (DURUM) - EN Ã–NEMLÄ° KAVRAM

### State Nedir?

State = Agent'Ä±n o anki durumunu tanÄ±mlayan bilgi

### Senin State'in

```python
state = [row, col, has_lover]
#        â”‚    â”‚     â”‚
#        â”‚    â”‚     â””â”€â”€ Lover aldÄ±n mÄ±? (0 veya 1)
#        â”‚    â””â”€â”€ SÃ¼tun pozisyonu (0-11)
#        â””â”€â”€ SatÄ±r pozisyonu (0-6)
```

### Kodda TanÄ±mÄ±

```python
self.observation_space = spaces.Box(
    low=np.array([0, 0, 0]),      # Minimum deÄŸerler
    high=np.array([6, 11, 1]),    # Maximum deÄŸerler
    shape=(3,),                   # 3 elemanlÄ± vektÃ¶r
    dtype=np.int32                 # Tam sayÄ±
)
```

### Toplam KaÃ§ State Var?

```
7 (satÄ±r) Ã— 12 (sÃ¼tun) Ã— 2 (lover durumu) = 168 state
```

**Neden has_lover state'in parÃ§asÄ±?**
- AynÄ± pozisyonda olsan bile, lover'lÄ± ve lover'sÄ±z durumlar **farklÄ±**!
- Lover alÄ±nca reward 6 katÄ±na Ã§Ä±kÄ±yor â†’ farklÄ± davranÄ±ÅŸ gerekiyor

---

## ğŸ® ACTION SPACE (AKSÄ°YON UZAYI)

### Aksiyonlar

```python
self.action_space = spaces.Discrete(3)
```

| Aksiyon | DeÄŸer | Ne Yapar? | Kod |
|---------|-------|-----------|-----|
| UP | 0 | YukarÄ± git | `row -= 1` |
| DOWN | 1 | AÅŸaÄŸÄ± git | `row += 1` |
| RIGHT | 2 | SaÄŸa git | `col += 1` |

**LEFT neden yok?**
- TasarÄ±m tercihi
- GÃ¶revi zorlaÅŸtÄ±rÄ±r (geri dÃ¶nÃ¼ÅŸ yok)
- Agent daha dikkatli plan yapmalÄ±

### GeÃ§ersiz Hareket

Duvara/obstacle'a Ã§arparsan:
- Pozisyon deÄŸiÅŸmez (aynÄ± yerde kalÄ±rsÄ±n)
- Ama step sayÄ±lÄ±r â†’ **-1 living cost** alÄ±rsÄ±n

---

## ğŸ’° REWARD STRUCTURE (Ã–DÃœL YAPISI)

### Reward Tablosu

| Durum | Normal Reward | Lover ile | Episode Biter? |
|-------|---------------|-----------|----------------|
| Her adÄ±m (living cost) | -1 | -1 | HayÄ±r |
| Goal'a ulaÅŸma | +100 | **+600** | Evet (terminated) |
| Danger'a dÃ¼ÅŸme | -100 | **-600** | Evet (terminated) |
| Lover bulma | +100 | - | HayÄ±r |
| Mini reward (R) | +1 | +1 | HayÄ±r |
| Max step (200) | 0 | 0 | Evet (truncated) |

### Living Cost Neden Var?

**OlmasaydÄ±:**
- 10 adÄ±mda goal = +100
- 1000 adÄ±mda goal = +100 (aynÄ±!)
- Agent kÄ±sa yolu Ã¶ÄŸrenmez

**Var olduÄŸunda:**
- 10 adÄ±mda goal = +100 - 10 = **+90 net**
- 50 adÄ±mda goal = +100 - 50 = **+50 net**
- Agent **EN KISA YOLU** Ã¶ÄŸrenir!

---

## ğŸ”„ ENVIRONMENT METODLARI

### 1. `__init__()` - BaÅŸlatma

```python
def __init__(self, num_rows=7, num_cols=12, ...):
    # Maze tanÄ±mla
    self.maze = np.array([...])
    
    # Ã–zel pozisyonlarÄ± parse et
    self.goal_states = [...]
    self.danger_states = [...]
    self.obstacle_states = [...]
    self.lover_state = [...]
    
    # Observation space tanÄ±mla
    self.observation_space = spaces.Box(...)
    
    # Action space tanÄ±mla
    self.action_space = spaces.Discrete(3)
```

### 2. `reset()` - Episode BaÅŸlat

```python
def reset(self):
    # Agent pozisyonunu baÅŸlangÄ±ca al
    self.position = self.agent_start.copy()
    
    # Lover flag'ini sÄ±fÄ±rla
    self.has_lover = False
    
    # State oluÅŸtur
    self.state = np.array([
        self.position[0],      # row
        self.position[1],       # col
        int(self.has_lover)     # has_lover (0 veya 1)
    ], dtype=np.int32)
    
    return self.state, {}
```

**Ne yapar?**
- Her yeni episode'da agent baÅŸlangÄ±ca dÃ¶ner
- Lover flag sÄ±fÄ±rlanÄ±r
- State oluÅŸturulur ve dÃ¶ndÃ¼rÃ¼lÃ¼r

### 3. `step(action)` - Aksiyon Al

**EN Ã–NEMLÄ° METOD!** Bu metod agent'Ä±n aksiyonunu alÄ±r, sonucu hesaplar.

```python
def step(self, action):
    # 1. Yeni pozisyonu hesapla
    new_row = self.position[0]
    new_col = self.position[1]
    
    if action == 0:  # UP
        new_row -= 1
    elif action == 1:  # DOWN
        new_row += 1
    elif action == 2:  # RIGHT
        new_col += 1
    
    # 2. GeÃ§erli pozisyon mu kontrol et
    if self._is_valid_position(new_row, new_col):
        self.position = [new_row, new_col]
    
    # 3. Reward hesapla
    reward = -self.living_cost  # Her adÄ±m -1
    
    # 4. Ã–zel durumlarÄ± kontrol et
    if self.position in self.goal_states:
        if self.has_lover:
            reward += self.goal_reward * self.lover_multiplier  # +600
        else:
            reward += self.goal_reward  # +100
        terminated = True
    elif self.position in self.danger_states:
        if self.has_lover:
            reward -= self.danger_penalty * self.lover_multiplier  # -600
        else:
            reward -= self.danger_penalty  # -100
        terminated = True
    elif self.position == self.lover_state:
        if not self.has_lover:
            reward += 100  # Ä°lk alÄ±ÅŸta +100
            self.has_lover = True
    elif self.position in self.reward_states:
        reward += self.mini_reward  # +1
    
    # 5. State'i gÃ¼ncelle
    self.state = np.array([
        self.position[0],
        self.position[1],
        int(self.has_lover)
    ], dtype=np.int32)
    
    # 6. Episode bitti mi kontrol et
    truncated = (self.step_count >= self.max_steps)
    done = terminated or truncated
    
    return self.state, reward, terminated, truncated, {}
```

**AdÄ±m AdÄ±m Ne Oluyor?**

1. **Pozisyon Hesapla**: Aksiyona gÃ¶re yeni pozisyon
2. **GeÃ§erlilik KontrolÃ¼**: Duvara/obstacle'a Ã§arptÄ± mÄ±?
3. **Reward Hesapla**: BaÅŸta -1 (living cost)
4. **Ã–zel Durumlar**:
   - Goal â†’ +100 veya +600 (lover varsa)
   - Danger â†’ -100 veya -600 (lover varsa)
   - Lover â†’ +100 (ilk alÄ±ÅŸta)
   - Mini reward â†’ +1
5. **State GÃ¼ncelle**: has_lover deÄŸiÅŸtiyse state deÄŸiÅŸir
6. **Episode Bitti mi?**: terminated (goal/danger) veya truncated (max step)

### 4. `render()` - GÃ¶rselleÅŸtir

```python
def render(self):
    if self.render_mode == "pygame":
        # Pygame ile gÃ¶rsel gÃ¶sterim
    elif self.render_mode == "human":
        # Text tabanlÄ± gÃ¶sterim
    elif self.render_mode == "ansi":
        # ANSI kodlarÄ± ile renkli gÃ¶sterim
```

---

# ASSIGNMENT 2: Q-LEARNING

## ğŸ¯ NE YAPIYORUZ?

Assignment 1'deki environment'ta agent'Ä± **Q-Learning** algoritmasÄ± ile eÄŸitiyoruz. Agent deneme-yanÄ±lma ile optimal policy'yi Ã¶ÄŸreniyor.

## ğŸ§  Q-LEARNING NEDÄ°R?

### Basit AÃ§Ä±klama

**Q-Learning** = "Bu durumda bu aksiyonu yaparsam, uzun vadede ne kadar kazanÄ±rÄ±m?"

### Q-Function

```
Q(s, a) = Beklenen Toplam Ã–dÃ¼l
```

- `s` = state (durum)
- `a` = action (aksiyon)
- `Q(s, a)` = Bu state-action Ã§iftinin deÄŸeri

### Ã–rnek

```
Q([3, 0, 0], RIGHT) = 85.5
```

Ne demek?
- State: SatÄ±r 3, SÃ¼tun 0, Lover yok
- Action: RIGHT (saÄŸa git)
- DeÄŸer: 85.5 (bu aksiyonun beklenen toplam getirisi)

---

## ğŸ“Š Q-TABLE

### Q-Table Nedir?

TÃ¼m state-action Ã§iftleri iÃ§in Q deÄŸerlerini tutan tablo.

### Senin Q-Table'Ä±n

```python
self.q_table = np.zeros((7, 12, 2, 3))
#                        â”‚   â”‚  â”‚  â”‚
#                        â”‚   â”‚  â”‚  â””â”€â”€ 3 aksiyon (UP, DOWN, RIGHT)
#                        â”‚   â”‚  â””â”€â”€ 2 lover durumu (0, 1)
#                        â”‚   â””â”€â”€ 12 sÃ¼tun
#                        â””â”€â”€ 7 satÄ±r
```

**Boyut:** 7 Ã— 12 Ã— 2 Ã— 3 = **504 Q deÄŸeri**

### BaÅŸlangÄ±Ã§

```python
self.q_table = np.zeros(...)  # TÃ¼m deÄŸerler = 0
```

Neden 0? Agent henÃ¼z hiÃ§bir ÅŸey bilmiyor!

---

## ğŸ”„ Q-LEARNING UPDATE (BELLMAN EQUATION)

### FormÃ¼l

```
Q(s, a) â† Q(s, a) + Î± Ã— [R + Î³ Ã— max Q(s', a') - Q(s, a)]
```

### ParÃ§alarÄ±

| Sembol | Ä°sim | Ne Yapar? |
|--------|------|-----------|
| `Q(s, a)` | Current Q | Åu anki Q deÄŸeri |
| `Î±` (alpha) | Learning Rate | Ne kadar hÄ±zlÄ± Ã¶ÄŸren |
| `R` | Reward | AnlÄ±k Ã¶dÃ¼l |
| `Î³` (gamma) | Discount Factor | Gelecek Ã¶dÃ¼llerin deÄŸeri |
| `max Q(s', a')` | Max Next Q | Sonraki state'teki en iyi Q |
| `R + Î³ Ã— max Q(s', a')` | Target Q | Hedef Q deÄŸeri |
| `Target - Current` | TD Error | Hata miktarÄ± |

### Kodda

```python
def update_q_value(self, state, action, reward, next_state, done):
    state_idx = self.get_state_index(state)
    next_state_idx = self.get_state_index(next_state)
    
    # Mevcut Q deÄŸeri
    current_q = self.q_table[state_idx][action]
    
    # Hedef Q deÄŸerini hesapla
    if done:
        # Terminal state: gelecek yok
        target_q = reward
    else:
        # Non-terminal: gelecek Ã¶dÃ¼lÃ¼ ekle
        max_next_q = np.max(self.q_table[next_state_idx])
        target_q = reward + self.discount_factor * max_next_q
    
    # Q-learning update
    self.q_table[state_idx][action] = current_q + self.learning_rate * (target_q - current_q)
```

### Ã–rnek Hesaplama

**Senaryo:**
- State: [3, 5, 0]
- Action: RIGHT
- Reward: -1 (living cost)
- Next state: [3, 6, 0]
- Max next Q: 80
- Gamma: 0.99

**Hesaplama:**
```
target_q = -1 + 0.99 Ã— 80 = -1 + 79.2 = 78.2
current_q = 50 (Ã¶rnek)
TD error = 78.2 - 50 = 28.2
new_q = 50 + 0.08 Ã— 28.2 = 50 + 2.26 = 52.26
```

---

## ğŸ² EPSILON-GREEDY POLICY

### Ne Demek?

**Epsilon-greedy** = Exploration (keÅŸif) ve Exploitation (sÃ¶mÃ¼rme) dengesi

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

```python
def choose_action(self, state, training=True):
    if training and np.random.rand() < self.epsilon:
        # Exploration: Random aksiyon seÃ§
        return self.env.action_space.sample()
    else:
        # Exploitation: En iyi aksiyonu seÃ§
        return int(np.argmax(self.q_table[state_idx]))
```

### Epsilon DeÄŸerleri

| Epsilon | Ne Yapar? |
|---------|-----------|
| Îµ = 1.0 | %100 random (baÅŸlangÄ±Ã§) |
| Îµ = 0.5 | %50 random, %50 greedy |
| Îµ = 0.1 | %10 random, %90 greedy |
| Îµ = 0.0 | %100 greedy (test modu) |

### Neden AzalÄ±yor?

- **BaÅŸta:** Ã‡ok explore et (ortamÄ± tanÄ±)
- **Sonra:** Exploit et (Ã¶ÄŸrendiÄŸini kullan)

**Sabit kalsa ne olur?**
- SÃ¼rekli random hareketler
- Ã–ÄŸrendiÄŸin policy'yi kullanamazsÄ±n
- Performans dÃ¼ÅŸÃ¼k kalÄ±r

---

## ğŸ“‰ EPSILON DECAY (AZALMA)

### Ä°ki YÃ¶ntem

#### 1. Multiplicative Decay (Basit)

```python
self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
# Ã–rnek: epsilon = 1.0 Ã— 0.995 = 0.995
```

#### 2. Reverse Sigmoid Decay (Senin Kodun)

```python
def reverse_sigmoid_decay(t, epsilon_initial, epsilon_min, k, t0):
    return epsilon_min + (epsilon_initial - epsilon_min) / (1 + np.exp(k * (t - t0)))
```

**Neden Reverse Sigmoid?**
- Smooth geÃ§iÅŸ (ani dÃ¼ÅŸÃ¼ÅŸ yok)
- S ÅŸeklinde azalma
- Daha kontrollÃ¼

**Parametreler:**
- `k`: Decay hÄ±zÄ± (ne kadar hÄ±zlÄ± azalacak)
- `t0`: Inflection point (en hÄ±zlÄ± azalmanÄ±n olduÄŸu episode)

---

## ğŸ‹ï¸ TRAINING LOOP

### Kod YapÄ±sÄ±

```python
def train(self, num_episodes=1000):
    for episode in range(num_episodes):
        # 1. Environment'Ä± reset et
        state, info = self.env.reset()
        done = False
        
        # 2. Episode boyunca dÃ¶ngÃ¼
        while not done:
            # 3. Aksiyon seÃ§ (epsilon-greedy)
            action = self.choose_action(state, training=True)
            
            # 4. Environment'ta aksiyonu al
            next_state, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            
            # 5. Q deÄŸerini gÃ¼ncelle (Bellman equation)
            self.update_q_value(state, action, reward, next_state, done)
            
            # 6. State'i gÃ¼ncelle
            state = next_state
        
        # 7. Epsilon'u azalt (decay)
        self.epsilon = reverse_sigmoid_decay(...)
```

### AdÄ±m AdÄ±m Ne Oluyor?

1. **Reset**: Agent baÅŸlangÄ±ca dÃ¶ner
2. **Action SeÃ§**: Epsilon-greedy ile aksiyon seÃ§
3. **Step**: Environment'ta aksiyonu al, reward al
4. **Update**: Q deÄŸerini gÃ¼ncelle (Ã¶ÄŸren!)
5. **Decay**: Epsilon'u azalt (daha az explore, daha Ã§ok exploit)

---

## ğŸ§ª TEST MODU

### Ne Fark Var?

```python
def test(self, num_episodes=10):
    for episode in range(num_episodes):
        while not done:
            # Greedy policy kullan (epsilon = 0)
            action = self.choose_action(state, training=False)
            # ...
```

**Fark:**
- `training=False` â†’ epsilon kullanÄ±lmaz
- Her zaman en iyi aksiyon seÃ§ilir (greedy)
- Random yok!

---

## ğŸ“ˆ GÃ–RSELLEÅTÄ°RME

### 1. Q-Table Visualization

```python
visualize_q_table(q_table, env, save_path="q_table.png")
```

**Ne gÃ¶sterir?**
- Her aksiyon iÃ§in Q deÄŸerlerini heatmap olarak
- YÃ¼ksek deÄŸerler = sarÄ±
- DÃ¼ÅŸÃ¼k deÄŸerler = mor

### 2. Policy Visualization

```python
visualize_policy(q_table, env, save_path="policy.png")
```

**Ne gÃ¶sterir?**
- Her state'te en iyi aksiyonu ok ile
- Optimal policy'yi gÃ¶rselleÅŸtirir

### 3. Training Curves

```python
plot_training_curves(training_stats, save_path="curves.png")
```

**Ne gÃ¶sterir?**
- Episode reward'larÄ±
- Success rate
- Epsilon deÄŸiÅŸimi

---

# KOD AÃ‡IKLAMALARI

## assignment2_qlearning.py - SatÄ±r SatÄ±r

### 1. QLearningAgent Class

```python
class QLearningAgent:
    def __init__(self, env, learning_rate=0.1, ...):
        self.env = env
        self.learning_rate = learning_rate  # Î± (alpha)
        self.discount_factor = discount_factor  # Î³ (gamma)
        self.epsilon = epsilon  # Exploration rate
        self.q_table = np.zeros((7, 12, 2, 3))  # Q-table baÅŸlat
```

**Ne yapar?**
- Agent'Ä± baÅŸlatÄ±r
- Hyperparameter'leri ayarlar
- Q-table'Ä± sÄ±fÄ±rlarla doldurur

### 2. choose_action()

```python
def choose_action(self, state, training=True):
    state_idx = self.get_state_index(state)
    
    if training and np.random.rand() < self.epsilon:
        return self.env.action_space.sample()  # Random
    else:
        return int(np.argmax(self.q_table[state_idx]))  # Greedy
```

**Ne yapar?**
- Epsilon olasÄ±lÄ±kla random aksiyon
- 1-epsilon olasÄ±lÄ±kla en iyi aksiyon

### 3. update_q_value()

```python
def update_q_value(self, state, action, reward, next_state, done):
    current_q = self.q_table[state_idx][action]
    
    if done:
        target_q = reward  # Terminal state
    else:
        max_next_q = np.max(self.q_table[next_state_idx])
        target_q = reward + self.discount_factor * max_next_q
    
    # Update
    self.q_table[state_idx][action] = current_q + self.learning_rate * (target_q - current_q)
```

**Ne yapar?**
- Bellman equation'Ä± uygular
- Q deÄŸerini gÃ¼nceller

### 4. train()

```python
def train(self, num_episodes=1000):
    for episode in range(num_episodes):
        state, info = self.env.reset()
        done = False
        
        while not done:
            action = self.choose_action(state, training=True)
            next_state, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            
            self.update_q_value(state, action, reward, next_state, done)
            state = next_state
        
        # Epsilon decay
        self.epsilon = reverse_sigmoid_decay(...)
```

**Ne yapar?**
- Episode'larÄ± Ã§alÄ±ÅŸtÄ±rÄ±r
- Her adÄ±mda Q deÄŸerini gÃ¼nceller
- Epsilon'u azaltÄ±r

---

# SINAV SORULARI VE CEVAPLARI

## ASSIGNMENT 1 SORULARI

### SORU 1: "Environment'Ä±nÄ± anlat."

**CEVAP:**
"7x12 boyutunda bir grid world yaptÄ±m. Ice Age temalÄ± - Scrat isimli sincap agent, palamuta (goal) ulaÅŸmaya Ã§alÄ±ÅŸÄ±yor. Engellerden kaÃ§Ä±nmasÄ±, tehlikeli bÃ¶lgelerden uzak durmasÄ± lazÄ±m. Bir de Scratte var, lover - onu alÄ±rsa reward 6 katÄ±na Ã§Ä±kÄ±yor ama risk de 6 kat artÄ±yor."

### SORU 2: "State neyi iÃ§eriyor?"

**CEVAP:**
"3 elemanlÄ± bir vektÃ¶r: row (satÄ±r 0-6), column (sÃ¼tun 0-11), has_lover (0 veya 1). Toplam 7Ã—12Ã—2 = 168 farklÄ± state var."

### SORU 3: "Neden has_lover state'in parÃ§asÄ±?"

**CEVAP:**
"Ã‡Ã¼nkÃ¼ lover alÄ±ndÄ±ÄŸÄ±nda reward 6 katÄ±na Ã§Ä±kÄ±yor. Yani agent aynÄ± pozisyonda olsa bile, lover'lÄ± ve lover'sÄ±z durumlar farklÄ± deÄŸerlere sahip. Agent'Ä±n farklÄ± davranmasÄ± gerekiyor - mesela lover aldÄ±ysan risk almaman lazÄ±m Ã§Ã¼nkÃ¼ Ã¶lÃ¼rsen 6 kat ceza alÄ±rsÄ±n."

### SORU 4: "Living cost neden -1?"

**CEVAP:**
"0 olsa agent acele etmezdi. 10 adÄ±mda da 1000 adÄ±mda da aynÄ± reward'Ä± alÄ±rdÄ±. -1 sayesinde her adÄ±m maliyetli, agent en kÄ±sa yolu Ã¶ÄŸreniyor."

### SORU 5: "step() metodunda ne oluyor?"

**CEVAP:**
"Ã–nce yeni pozisyonu hesaplÄ±yorum. GeÃ§erli mi kontrol ediyorum. Reward hesaplÄ±yorum - baÅŸta -1 living cost. Sonra Ã¶zel durumlarÄ± kontrol ediyorum - goal, danger, lover, mini reward. State'i gÃ¼ncelliyorum, Ã¶zellikle has_lover deÄŸiÅŸtiyse. Episode bitti mi kontrol ediyorum."

---

## ASSIGNMENT 2 SORULARI

### SORU 1: "Q-Learning algoritmasÄ±nÄ± anlat."

**CEVAP:**
"Off-policy TD control algoritmasÄ±. Agent epsilon-greedy ile aksiyon seÃ§iyor - bazen random bazen en iyi. Her adÄ±mda Q deÄŸerini gÃ¼ncelliyor: Q(s,a) = Q(s,a) + Î± Ã— [R + Î³ Ã— maxQ(s') - Q(s,a)]. Zamanla epsilon azalÄ±yor, daha Ã§ok exploit ediyor."

### SORU 2: "Q-table boyutu ne?"

**CEVAP:**
"7 Ã— 12 Ã— 2 Ã— 3 = 504 deÄŸer. 7 satÄ±r, 12 sÃ¼tun, 2 lover durumu, 3 aksiyon."

### SORU 3: "Bellman equation'Ä± yaz ve aÃ§Ä±kla."

**CEVAP:**
"Q(s, a) = R + Î³ Ã— max Q(s', a'). Yani bir state-action'Ä±n deÄŸeri, anlÄ±k reward artÄ± indirimli gelecek deÄŸerine eÅŸit. R anlÄ±k reward, Î³ discount factor, max Q(s', a') sonraki state'teki en iyi Q deÄŸeri."

### SORU 4: "Epsilon-greedy ne demek?"

**CEVAP:**
"Exploration-exploitation dengesi iÃ§in kullandÄ±ÄŸÄ±m strateji. Epsilon olasÄ±lÄ±kla random aksiyon yapÄ±yorum (explore), 1-epsilon olasÄ±lÄ±kla en iyi aksiyonu (exploit). BaÅŸta epsilon=1 yani full random, sonra yavaÅŸ yavaÅŸ azalÄ±yor."

### SORU 5: "Off-policy ne demek?"

**CEVAP:**
"DavrandÄ±ÄŸÄ±m policy ile Ã¶ÄŸrendiÄŸim policy farklÄ±. Ben epsilon-greedy ile davranÄ±yorum ama Ã¶ÄŸrendiÄŸim greedy/optimal policy. Update'te max Q kullanÄ±yorum, bu greedy policy. DavranÄ±rken bazen random yapÄ±yorum, bu epsilon-greedy. Ä°kisi farklÄ± olduÄŸu iÃ§in off-policy."

### SORU 6: "Alpha deÄŸerin ne? Neden bu deÄŸeri seÃ§tin?"

**CEVAP:**
"0.08 kullandÄ±m. DÃ¼ÅŸÃ¼k bir deÄŸer - yavaÅŸ ama stabil Ã¶ÄŸrenme saÄŸlÄ±yor. Ã‡ok yÃ¼ksek olsa Q deÄŸerleri salÄ±nÄ±m yapardÄ±, converge etmezdi. 0.08 ile yavaÅŸ yavaÅŸ ama gÃ¼venli Ã¶ÄŸreniyor."

### SORU 7: "Gamma 0 olsa ne olurdu?"

**CEVAP:**
"Agent miyop olurdu, sadece anlÄ±k reward'a bakardÄ±. Her adÄ±m -1 olduÄŸu iÃ§in hiÃ§bir yere gitmek istemezdi. Goal'Ä±n +100 olduÄŸunu gÃ¶remezdi Ã§Ã¼nkÃ¼ gelecek deÄŸeri sÄ±fÄ±r sayardÄ±."

### SORU 8: "Reverse sigmoid decay ne?"

**CEVAP:**
"Normal decay'de epsilon = epsilon Ã— 0.995 gibi Ã§arparak azaltÄ±yorsun, Ã¼stel azalma. Reverse sigmoid'de S ÅŸeklinde azalÄ±yor - baÅŸta yavaÅŸ, ortada hÄ±zlÄ±, sonda tekrar yavaÅŸ. Daha smooth bir geÃ§iÅŸ saÄŸlÄ±yor, ani dÃ¼ÅŸÃ¼ÅŸler yok."

### SORU 9: "Test'te epsilon kaÃ§?"

**CEVAP:**
"SÄ±fÄ±r. Test'te exploration yapmÄ±yorum, sadece Ã¶ÄŸrendiÄŸim policy'yi kullanÄ±yorum. Her state'te argmax Q alÄ±yorum, yani en iyi aksiyonu seÃ§iyorum. Random yok."

### SORU 10: "update_q_value() metodunda ne oluyor?"

**CEVAP:**
"Ã–nce mevcut Q deÄŸerini alÄ±yorum. Sonra hedef Q deÄŸerini hesaplÄ±yorum - eÄŸer terminal state ise sadece reward, deÄŸilse reward + gamma Ã— max next Q. Sonra TD error'Ã¼ hesaplayÄ±p (target - current), bunu learning rate ile Ã§arpÄ±p mevcut Q'ya ekliyorum."

---

## ğŸ¯ HIZLI REFERANS

### FormÃ¼ller

| FormÃ¼l | AÃ§Ä±klama |
|--------|----------|
| `Q(s,a) â† Q(s,a) + Î±[R + Î³ max Q(s',a') - Q(s,a)]` | Q-Learning Update |
| `Îµ-greedy: P(random) = Îµ, P(greedy) = 1-Îµ` | Exploration Policy |
| `V(s) = max Q(s, a)` | Value Function |

### DeÄŸerler

| Parametre | DeÄŸer | AmaÃ§ |
|-----------|-------|------|
| Grid | 7Ã—12 | Environment boyutu |
| State | (row, col, has_lover) | 3 boyutlu observation |
| Actions | 3 (UP, DOWN, RIGHT) | Hareket seÃ§enekleri |
| Î± | 0.08 | YavaÅŸ-stabil Ã¶ÄŸrenme |
| Î³ | 0.995 | Uzun vadeli planlama |
| Îµ | 1.0 â†’ 0.1 | Azalan exploration |
| Living cost | -1 | KÄ±sa yol teÅŸviki |
| Goal reward | +100 (Ã—6 with lover) | Hedefe ulaÅŸma |

---

## âœ… SINAV Ã–NCESÄ° KONTROL LÄ°STESÄ°

- [ ] Environment yapÄ±sÄ±nÄ± anladÄ±m (7Ã—12, Ã¶zel hÃ¼creler)
- [ ] State'in ne olduÄŸunu biliyorum (row, col, has_lover)
- [ ] Action space'i biliyorum (3 aksiyon: UP, DOWN, RIGHT)
- [ ] Reward yapÄ±sÄ±nÄ± biliyorum (living cost, goal, danger, lover)
- [ ] Q-table'Ä±n boyutunu biliyorum (7Ã—12Ã—2Ã—3 = 504)
- [ ] Bellman equation'Ä± yazabilirim
- [ ] Epsilon-greedy'yi aÃ§Ä±klayabilirim
- [ ] Off-policy kavramÄ±nÄ± biliyorum
- [ ] Alpha, gamma, epsilon'u aÃ§Ä±klayabilirim
- [ ] Training loop'u anladÄ±m
- [ ] Test modunun farkÄ±nÄ± biliyorum

---

**BAÅARILAR! ğŸš€**

